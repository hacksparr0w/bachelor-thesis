\chapter{Molecular Dynamics}

Molecular dynamics simulations is an umbrella term for a class of
computational methods used to model and analyze the physical behavior of
systems of atoms and molecules. MD allows one to monitor the full time
evolution of a system, allowing for deep examination of the dynamics of
atomic-level phenomena that cannot be observed directly.
Computer simulations applied to condensed matter systems began their
development as early as the 1950s, when two of the pillars of molecular
simulation were introduced, namely the Monte Carlo (MC) sampling technique and
the molecular dynamics method. In 1964, the first realistic MD simulation was
developed by Rahman, who came up with a realistic model of liquid Argon.
Rahman used the Lennard–Jones pair-wise additive potential and showed, that
MD simulations with smooth potentials were possible.

Around the same time, Verlet proposed a stable numerical integration
algorithm that is still very popular in modern MD software. We will discuss
the Verlet integration algorithm in further chapters of this thesis. Verlet
also invented a time-saving algorithm, the Verlet neighbour list.

A great leap forward in the MD methology happened in 1971, when Rahman and
Stillinger published an MD study on modeling a realistic system of liquid
water, a system composed of molecules, not just individual atoms. The
significant results of their work prompted a multinational group of scientists
centred around Berendsen at CECAM to try using MD simulations for examining
biomolecules. The first MD simulation of a simple protein was due to Karplus
and collaborators, and appeared shortly after, in 1977. In 2013, the Nobel
Prize for Chemistry was awarded to Warshel, Levitt, and Karplus for their work
on computer simulations in biochemistry, which was built upon the efforts of
many researchers who had previously worked on simulating biomolecules.

Another inportant development took place in 1980. In this year, Anderson
published a paper that described how to extend MD to enable it to sample the
isoenthalpic (constant pressure) ensemble. The standard molecular dynamics
algorithm was designed to simulate the behavior of a system of particles at
constant energy, or in the microcanonical ensemble, because the Newton's
equations of motion conserve energy. It was not straightforward to modify the
MD algorithm to sample systems under different, more experimentally relevant
conditions. Andersen's extensions for sampling the isoenthalpic ensemble
inspired the question of whether it was possible to use MD to sample the
canonical ensemble as well. Nosé, building on Andersen's work, introduced a
new variable that linked the kinetic energy of the atoms to the external
temperature, resulting in dynamics that sample the desired ensemble. This
approach is known as the Nosé–Hoover thermostat, which is often used in a
modified form called the Hoover thermostat.

In 1985, Car and Parrinello published a groundbreaking paper in Physical
Review Letters that described a method for combining MD with density
functional theory (DFT) calculations of electronic structure. This approach
eliminated the need for a potential model, as energy, forces, and stress
could be calculated directly from the electronic structure. The Car–Parrinello
method allowed for the simulation of processes that involve bond formation or
breaking and was the first to demonstrate that it is possible to combine
finite temperature simulations with ground-state electronic structure
calculations. This method also served as a bridge between the simulation
community, which typically has a background in statistical mechanics, and the
solid-state physics and quantum chemistry communities, which focus on
electronic structure calculations at zero temperature.

During the 1980s and 1990s, the use of molecular simulations in condensed
matter research became more widespread, due in part to previous successes in
this field and also to the increasing availability and power of computers.

Modern MD methodology is frequently used to refine the three-dimensional
structures of proteins and other large molecules, to study atomic-level
phenomena that cannot be directly observed, such as thin-film growth and ion
implantation, and to investigate the physical properties of nanotechnological
devices that cannot yet be manufactured. In 2015, for example, MD simulation
has been reported for pharmacophore development and drug design.

\subsection{Classical Methods}

The classical MD implementation uses the so-called "ball and sticks" model,
where atoms and molecules are treated as soft balls and their bonds are
represented by elastic sticks. The laws of classical mechanics define the
dynamics of the entire system.

Each particle in an MD simulation has its own position vector
$\mathbf{r}_i(t) = (x_i(t), y_i(t), z_i(t))$. A particle usually corresponds
to an atom, although it may represent any simulable entity of interest that
can be conveniently described by an interaction law. By Newton's second law
the motion of each particle must obey the following relation

\begin{equation}
  \mathbf{F}_i = m_i \diff[2]{\mathbf{r}_i}{t},
\end{equation}

\noindent where $m_i$ is the mass of $i$-th particle and $\mathbf{F}_i$ is the
force acting upon $i$-th particle. Interaction laws are usually specified by
a potential function $U(\mathbf{r}_1, \dots, \mathbf{r}_N)$, which represents
the potential energy of $N$ interacting particles as a function of their
positions. Given the potential, the force acting upon $i$-th atom is
determined by the gradient with respect to particle displacements

\begin{equation}
  \mathbf{F}_i = - \nabla_{\mathbf{r}_i} U(\mathbf{r}_1, \dots, \mathbf{r}_N)
  = - \left(
    \frac{\partial U}{\partial x_i},
    \frac{\partial U}{\partial y_i},
    \frac{\partial U}{\partial z_i}
  \right).
\end{equation}

\noindent Let's briefly talk about the meaning and form of the potential $U$
in MD simulations. Any quantum-chemistry textbook would insist, that in order
to appropriately examine a behavior of molecule, we can't just look at its
individual atoms. The quantum-mechanical lense reveals, that when atoms bond
into molecules, their electron orbitals interact in complex ways, giving rise
to non-trivial molecule orbitals. These electronic clouds that span multiple
atoms then determine molecule's interactions with other particles. This paints
molecules as a very complicated quantum systems, where electrons and nuclei
are interacting together in an intricate manner. It turns out, however, that
to a very good approximation, known as the Born–Oppenheimer adiabatic
approximation and based on the difference in mass between nuclei and
electrons, the electronic and nuclear problems can be separated. According to
this approximation, we can presume that the electron clouds equilibrate
quickly for each instanteneous configuration of the heavy nuclei. The nuclei
then move in the field created by the average electron densities.
This allows us to consider the concept of a potential energy surface, which
controls the movement of the nuclei without taking explicit account of the
electrons. Given the potential energy surface, we may use classical mechanics
to follow the dynamics of the nuclei. Rather than solving the
quantum-mechanical problem, we can solve a classic-mechanical problem, in
which the effect of the electrons on nuclei is expressed by en empirical
potential. It can be very challenging to identify a potential function that
accurately represents an energy surfaces of a system, but doing so greatly
simplifies the computational process. Atomic force field models and the
classical MD are based on empirical potentials with a specific functional
form, representing the physics and chemistry of the systems of
interest. The following equation is an example of such a force field, used in
biosystem simulations

%
% TODO: Include a brief discussion of MD limitations, scaling, etc.
%

\begin{equation}
\begin{alignedat}{2}
  U(\mathbf{r}_1, \dots, \mathbf{r}_N) = &
  \sum_\text{bonds} \frac{a_i}{2} (l_i - l_{i0})^2
  + \sum_\text{angles} \frac{b_i}{2} (\theta_i - \theta_{i0})^2 \\
  &+ \sum_\text{torsions} \frac{c_i}{2} \left[
    1 + \cos(n \omega_i - \gamma_i)
  \right] \\
  &+ \sum_\text{atom pairs} 4 \varepsilon_{ij} \left[
    \left(\frac{\sigma_{ij}}{r_{ij}}\right)^{12}
    - \left(\frac{\sigma_{ij}}{r_{ij}}\right)^{6}
  \right] \\
  &+ \sum_\text{atom pairs} k \frac{q_i q_j}{r_{ij}}.
\end{alignedat}
\end{equation}

The covalent character of the system is defined by the first three terms of
the system, where the summation indices run over all the bonds, angles and
torsions. In contrast, the last two terms are only defined by atom pairs,
with $q_i q_j$ being the product of their charges and $r_{ij} = |r_i - r_j|$
is the distance between the two atoms in the pair. The first two terms give
energies of deformations of the bond lengths $l_i$ and bond angles $\theta_i$
from their respective equilibrium values $l_{i0}$ and $\theta_{i0}$ with force
constants $a_i$ and $b_i$. These two terms model the correct chemical
structure, but prevent more complicated chemical phenomena like bond breaking.
Rotations around the chemical bond are described by the third therm, which is
priodic with periodicity determined by $n$ and heights of rotational barriers
defined by $c_i$. The forth term represents the van der Waals repulsive and
attractive interatomic forces in the form of the Lennard–Jones 12-6 potential.
The last term is the Coulomb electrostatic potential. Some effects due to
specific environments can be accounted for by properly adjusting partial
charges $q_i$ and effective value of the constant $k$ as well as the van der
Waals parameters $\varepsilon_{ij}$ and $\sigma_{ij}$.

We now have a full mathematical description of the problem at hand. Due to the
many-body nature of the problem, it is out of question to solve it
analytically, thus it has to be discretized and solved numerically with a
computer. First, we need to specify the initial conditions of the system,
that is, the initial positions $\mathbf{r}_{i0}$ and initial velocities
$\mathbf{v}_{i0}$ of the particles in the system. Then we have to use a
numerical integrator to continually make finite time interval steps and find
the successive values of positions $\mathbf{r}_i(t)$ and velocities
$\mathbf{v}_{i}(t)$ in the time evolution of the system.

\subsection{\textit{Ab Initio} Methods}



\subsection{Density Functional Theory}



\subsection{Machine Learning}



\subsection{Deep Potential Molecular Dynamics}
